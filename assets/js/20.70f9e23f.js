(window.webpackJsonp=window.webpackJsonp||[]).push([[20],{410:function(e,t,n){"use strict";n.r(t);var o=n(9),s=Object(o.a)({},(function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[n("p",[e._v("As I am a fan of HBR, I am gonna work on finding the frequency of an HBR Article here.")]),e._v(" "),n("p",[e._v("To keep it simplified, I haven’t used any efficiency in the code.\nFeel free to optimize the code wherever it’s possible.")]),e._v(" "),n("p",[e._v("For a sample, we have chosen the article below:")]),e._v(" "),n("p",[n("a",{attrs:{href:"https://hbr.org/2011/09/learning-to-live-with-complexity",target:"_blank",rel:"noopener noreferrer"}},[e._v("Learning to Live with Complexity"),n("OutboundLink")],1)]),e._v(" "),n("p",[e._v("To simplify the process, I have saved the article content in the text file (article1.txt).")]),e._v(" "),n("p",[n("strong",[e._v("Get the content from the text:")])]),e._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[e._v("content = ''\nwith open(filename, encoding=\"utf8\") as f:\n     for line in f:\n     content = content + line.lower()\n")])])]),n("p",[e._v("The above code will get the content of the file (article text in our case). I have used a simple file open and string concatenation method here.")]),e._v(" "),n("p",[n("strong",[e._v("Word Tokenize:")])]),e._v(" "),n("p",[e._v("The word_tokenize method will get the word as tokens")]),e._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[e._v("tokens = word_tokenize(content)\n")])])]),n("p",[e._v("The above code will get the tokens from the content. It may have stop words and other unnecessary words which we don’t need. We will clean them up soon.")]),e._v(" "),n("p",[n("strong",[e._v("Token Clean up Process:")])]),e._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[e._v("clean_tokens = []\nstop_words = stopwords.words('english')\nfor token in tokens:\n    # ignore string less than 4 characters\n    if(len(token) < 4):\n        continue\n    if(token in stopwords.words('english')):\n        continue\n    token = porter_stemmer.stem(token)\n    \n    clean_tokens.append(token)\n")])])]),n("p",[e._v("At first, we have removed the token who has less than 4 characters as they are not necessary. Then we have checked for stop words. If the token has stop words, we ignore them as well.")]),e._v(" "),n("p",[e._v("Finally, we have stemmed by using PoterStemmer.")]),e._v(" "),n("p",[e._v("What is PorterStemmer?")]),e._v(" "),n("p",[e._v("PorterStemmer will help us to get the root word for all words. For “keep, keeping and kept”, it will return “keep”")]),e._v(" "),n("p",[e._v("We can use PorterStemmber like below:")]),e._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[e._v("from nltk.stem import PorterStemmer\nporter_stemmer = PorterStemmer()\ntoken = porter_stemmer.stem(token)\n")])])]),n("p",[n("strong",[e._v("Frequency and Plot:")])]),e._v(" "),n("p",[e._v("Once we get the clean tokens, it is time to find the frequency distribution. This can be done with FreqDist in NLTK like below:")]),e._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[e._v("freq = nltk.FreqDist(clean_tokens)\nfreq.plot(20, cumulative=False)\n")])])]),n("p",[n("img",{attrs:{src:"https://miro.medium.com/max/700/1*kFTdRJqoMh8RdiGppF6gDg.png",alt:"Graph"}})]),e._v(" "),n("p",[n("strong",[e._v("Results")])]),e._v(" "),n("p",[e._v("It seems the words “system” and “complex” are used majorly comparing “event” or “inform”. This experiment is just a start. We can come up with many analysis by using NLTK!")]),e._v(" "),n("p",[n("strong",[e._v("Complete Code")])]),e._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[e._v("import os \nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\n\nporter_stemmer = PorterStemmer()\n\ndef get_dir():\n    dir_path = os.path.dirname(os.path.realpath(__file__))\n    return dir_path\n\ndef get_frequency(filename):\n\n    content = ''\n    with open(filename, encoding=\"utf8\") as f:\n        for line in f:\n            #print(line, end = '') \n            content = content + line.lower()\n\n    #print(content)\n\n    tokens = word_tokenize(content)\n\n    freq = nltk.FreqDist(tokens)\n\n    clean_tokens = [] \n\n    stop_words = stopwords.words('english')\n\n    for token in tokens:\n\n        # ignore string less than 4 characters\n        if(len(token) < 4):\n            continue\n\n        if(token in stopwords.words('english')):\n            continue\n\n        token = porter_stemmer.stem(token)\n\n        clean_tokens.append(token)\n\n    freq = nltk.FreqDist(clean_tokens)\n    freq.plot(20, cumulative=False)\n\n            \nget_frequency(get_dir() + \"/article1.txt\")\n")])])]),n("p",[n("strong",[e._v("Final Thoughts")])]),e._v(" "),n("p",[e._v("We have used a very simple method to show the distribution. As this article is focusing on students and newcomers, we haven’t used any complex logic or lambda function.")]),e._v(" "),n("p",[e._v("If you have any questions/improvements/issues, please let me know. If you like the article, please hit the like icon.")])])}),[],!1,null,null,null);t.default=s.exports}}]);