(window.webpackJsonp=window.webpackJsonp||[]).push([[34],{426:function(e,t,a){"use strict";a.r(t);var s=a(9),n=Object(s.a)({},(function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("p",[e._v("Newest PyTorch Lightning release includes the final API with better data decoupling, shorter logging syntax and tons of bug fixes")]),e._v(" "),a("p",[e._v("We’re happy to release PyTorch Lightning 0.9.0 today, which contains many great new features, more bug fixes than any release we ever had, but most importantly it introduced our mostly final API changes!")]),e._v(" "),a("p",[e._v("Lightning is being adopted by top researchers and AI labs around the world, and we are working hard to make sure we provide a smooth experience and support for all the latest best practices.")]),e._v(" "),a("p",[e._v("In this release, we are introducing two new major (and last) API changes:\nLightningDataModules\n"),a("img",{attrs:{src:"https://miro.medium.com/max/700/1*vAMTBrmXPUtyvqR72i-wew.gif",alt:"Image"}}),e._v("\nLightning is all about making your code more readable and structured.")]),e._v(" "),a("p",[e._v("We decouple the model architecture from engineering, and we continue to do the same with data. To make sharing and reusing data splits and transforms across projects, we created LightningDataModules.")]),e._v(" "),a("p",[e._v("A LightningDataModule is a shareable, reusable class that encapsulates all the steps needed for training:")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("Download / tokenize / process.\nClean and save to disk for reuse.\nLoad inside Dataset in memory or just-in-time.\nApply transforms (rotate, tokenize, etc…).\nWrap inside a DataLoader.\n")])])]),a("p",[e._v("LightningDataModules can be shared and used anywhere:")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("1.import pytorch_lightning as pl\n2.from torch.utils.data import random_split, DataLoader\n3.\n4.# Note - you must have torchvision installed for this example\n5.from torchvision.datasets import MNIST\n6.from torchvision import transforms\n7.\n8.\n9.class MNISTDataModule(pl.LightningDataModule):\n10.\n11.    def __init__(self, data_dir: str = './'):\n12.       super().__init__()\n13.       self.data_dir = data_dir\n14.        self.transform = transforms.Compose([\n15.            transforms.ToTensor(),\n16.            transforms.Normalize((0.1307,), (0.3081,))\n17.        ])\n18.\n19.        # self.dims is returned when you call dm.size()\n20.        # Setting default dims here because we know them.\n21.        # Could optionally be assigned dynamically in dm.setup()\n22.        self.dims = (1, 28, 28)\n23.\n24.    def prepare_data(self):\n25.        # download\n26.        MNIST(self.data_dir, train=True, download=True)\n27.        MNIST(self.data_dir, train=False, download=True)\n28.\n29.    def setup(self, stage=None):\n30.\n31.        # Assign train/val datasets for use in dataloaders\n32.        if stage == 'fit' or stage is None:\n33.    mnist_full = MNIST(self.data_dir, train=True, transform=self.transform)       \n34.            self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n35.\n36.            # Optionally...\n37.            # self.dims = tuple(self.mnist_train[0][0].shape)\n38.\n39.        # Assign test dataset for use in dataloader(s)\n40.        if stage == 'test' or stage is None:\n41.            self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n42.\n43.            # Optionally...\n44.           # self.dims = tuple(self.mnist_test[0][0].shape)\n45.\n46.    def train_dataloader(self):\n47.        return DataLoader(self.mnist_train, batch_size=32)\n48.\n49.    def val_dataloader(self):\n50.        return DataLoader(self.mnist_val, batch_size=32)\n51.\n52.    def test_dataloader(self):\n53.        return DataLoader(self.mnist_test, batch_size=32)      \n54.\n55. model = LitClassifier()\n56. trainer = Trainer()\n57. mnist = MNISTDataModule()\n58. trainer.fit(model, mnist)\n59. view raw\n")])])]),a("p",[e._v("In this video Nate Raw, DL research engineer at PyTorch Lightning, walks you step by step:")]),e._v(" "),a("p",[e._v("https://youtu.be/L---MBeSXFw")]),e._v(" "),a("p",[e._v("You can check out the docs on the new DataModules here.\nStep results")]),e._v(" "),a("p",[e._v("We added to Lightning two new results objects: TrainResult and EvalResult. They are fancy dictionary objects to hold outputs from train/eval/test steps. They are meant to control where and when to log and how synchronization is done across accelerators:")]),e._v(" "),a("p",[e._v("Use TrainResult to auto log from training_step:")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("# without val/test loop you can add model checkpoint or early stop\ndef training_step(self, batch, batch_idx):\n  x, y = batch\n  y_hat = self(x)\n  loss = F.cross_entropy(y_hat, y)\n  result = pl.TrainResult(loss, early_stop_on=loss, checkpoint_on=loss)\n  result.log('train_loss', loss)\n  return result\n")])])]),a("p",[e._v("The ‘train_loss’ we added to TrainResult will generate automatic tensorboard logs (you can also use any of the other loggers we support):\n"),a("img",{attrs:{src:"https://miro.medium.com/max/700/1*PS99ZwxC0dfliCA6Tt9izA.png",alt:"Image"}}),e._v("\nTrainResult default is to log every step of training.")]),e._v(" "),a("p",[e._v("Use EvalResult to auto log from validation_step or test_step:")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("def validation_step(self, batch, batch_idx):\n  x, y = batch\n  y_hat = self(x)\n  loss = F.cross_entropy(y_hat, y)\n  result = EvalResult()\n  result.log('val_loss', loss)\n  return result\n\ndef test_step(self, batch, batch_idx):\n  x, y = batch\n  y_hat = self(x)\n  loss = F.cross_entropy(y_hat, y)\n  result = EvalResult()\n  result.log('test_loss', loss)\n  return result\n")])])]),a("p",[e._v("EvalResult default is to log every epoch end.\n"),a("img",{attrs:{src:"https://miro.medium.com/max/700/1*K5zsUdjJdmC9iM_bY8RLzA.png",alt:"Feature"}}),e._v("\nSync across devices")]),e._v(" "),a("p",[e._v("When training on multiple GPUs/CPUs/TPU cores, you can calculate the global mean of a logged metric as follows:")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("result.log('train_loss', loss, sync_dist=True)\n")])])]),a("p",[e._v("For more logging options, check out our docs.")]),e._v(" "),a("p",[e._v("A few other highlights of 0.9 include:")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("PyTorch 1.6 support\nAdded saving test predictions on multiple GPUs\nAdded support to export a model to ONNX format\nMore sklearn metrics, SSIM, BLEU\nAdded SyncBN for DDP\nSupport for remote directories via gfile\n")])])]),a("p",[e._v("Read the full release notes here.")]),e._v(" "),a("p",[e._v("We also upgraded our docs with some videos that illustrate core Lightning features in seconds! Check them out, and let us know what you’d like to see next!")]),e._v(" "),a("p",[e._v("We want to thank all our devoted contributors for their hard work, and to the community for all your help. We definitely wouldn’t get here without you. Try it out, share your projects on our #slack, and stay tuned for our next release- 1.0!")])])}),[],!1,null,null,null);t.default=n.exports}}]);