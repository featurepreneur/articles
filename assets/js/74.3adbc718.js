(window.webpackJsonp=window.webpackJsonp||[]).push([[74],{464:function(e,a,t){"use strict";t.r(a);var s=t(9),r=Object(s.a)({},(function(){var e=this,a=e.$createElement,t=e._self._c||a;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("p",[e._v("Disclaimer: Shadow Article")]),e._v(" "),t("p",[t("strong",[e._v("Topics Discussed:")])]),e._v(" "),t("ul",[t("li",[e._v("Tensors")]),e._v(" "),t("li",[e._v("Basic Operations. (Inline, Tensor Indexing, Slicing)")]),e._v(" "),t("li",[e._v("Numpy-PyTorch Bridge")]),e._v(" "),t("li",[e._v("PyTorch-Numpy Bridge")]),e._v(" "),t("li",[e._v("Variable")]),e._v(" "),t("li",[e._v("Gradients")])]),e._v(" "),t("p",[t("strong",[e._v("What is PyTorch?")]),e._v("\nIt’s a Python based package for serving as a replacement of Numpy and to provide flexibility as a Deep Learning Development Platform.")]),e._v(" "),t("p",[t("strong",[e._v("Why PyTorch?")]),e._v("\nI encourage you to read Fast AI’s blog post for the reason of the course’s switch to PyTorch.\nOr simply put:\nDynamic Graphs\nMore intuitive than TF (Personal View)")]),e._v(" "),t("p",[t("strong",[e._v("Tensors")]),e._v("\nTensors are similar to numpy’s ndarrays, with the addition being that Tensors can also be used on a GPU to accelerate computing.\nTensors are multi dimensional Matrices.")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("torch.Tensor(x, y)\n")])])]),t("p",[e._v("This will create a X by Y dimensional Tensor that has been instantiated with random values.\nTo Create a 5x3 Tensor with values randomly selected from a Uniform Distribution between -1 and 1,")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("torch.Tensor(5, 3).uniform_(-1, 1)\n")])])]),t("p",[e._v("Tensors have a size attribute that can be called to check their size")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("print(x.size())\n")])])]),t("p",[t("strong",[e._v("Operations")]),e._v("\nPyTorch supports various Tensor Functions with different syntax:\nConsider Addition:")]),e._v(" "),t("p",[e._v("Normal Addition")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("y = torch.rand(5, 3)\nprint(x + y)\n")])])]),t("p",[e._v("Getting Result in a Tensor")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("result = torch.Tensor(5, 3)\ntorch.add(x, y, out=result)\n")])])]),t("p",[e._v("In Line")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("y.add_(x)\n")])])]),t("p",[e._v("Inline functions are denoted by an underscore following their name. Note: These have faster execution time (With a higher memory complexity tradeoff)")]),e._v(" "),t("p",[e._v("All Numpy Indexing, Broadcasting and Reshaping functions are supported\nNote: PyTorch doesn’t support a negative hop so [::-1] will result in an error")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("print(x[:, 1])\ny = torch.randn(5, 10, 15)\nprint(y.size())\nprint(y.view(-1, 15).size())\n")])])]),t("p",[t("strong",[e._v("Types of Tensors")])]),e._v(" "),t("p",[e._v("PyTorch supports various types of Tensors:")]),e._v(" "),t("p",[e._v("Note: Be careful when working with different Tensor Types to avoid type errors")]),e._v(" "),t("p",[e._v("Types supported:")]),e._v(" "),t("p",[e._v("32-bit (Float + Int)")]),e._v(" "),t("p",[e._v("64-bit (Float + Int)")]),e._v(" "),t("p",[e._v("16-bit (Float + Int)")]),e._v(" "),t("p",[e._v("8-bit (Signed + Unsigned)")]),e._v(" "),t("p",[t("strong",[e._v("Numpy Bridge")])]),e._v(" "),t("p",[e._v("Converting a torch Tensor to a numpy array and vice versa is a breeze.")]),e._v(" "),t("p",[e._v("Note: The torch Tensor and numpy array will share their underlying memory locations, and changing one will change the other.")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("a = torch.ones(5)\nb = a.numpy()\n")])])]),t("p",[t("strong",[e._v("CUDA Tensors")])]),e._v(" "),t("p",[e._v("Moving the Tensors to GPU can be done as:")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("if torch.cuda.is_available():\n    x = x.cuda()\n    y = y.cuda()\n    x + y\n")])])]),t("p",[t("strong",[e._v("Autograd: automatic differentiation")])]),e._v(" "),t("p",[t("img",{attrs:{src:"https://miro.medium.com/max/368/1*COBTDf1oef8r7lbPxcxQcw.png",alt:"Autograd variable"}})]),e._v(" "),t("p",[e._v("Central to all neural networks in PyTorch is the autograd package. Let’s first briefly visit this, and we will then go to training our first neural network.")]),e._v(" "),t("p",[e._v("The autograd package provides automatic differentiation for all operations on Tensors. It is a define-by-run framework, which means that your backprop is defined by how your code is run, and that every single iteration can be different.")]),e._v(" "),t("p",[e._v("Let us see this in more simple terms with some examples.")]),e._v(" "),t("p",[t("strong",[e._v("Variable")])]),e._v(" "),t("p",[e._v("autograd.Variable is the central class of the package. It wraps a Tensor, and supports nearly all of operations defined on it. Once you finish your computation you can call .backward() and have all the gradients computed automatically.")]),e._v(" "),t("p",[e._v("You can access the raw tensor through the .data attribute, while the gradient w.r.t. this variable is accumulated into .grad.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://miro.medium.com/max/368/0*4NtOmdyorhdH9DGl.png",alt:"PyTorch variable"}})]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("x_data = [1.0, 2.0, 3.0]\ny_data = [2.0, 4.0, 6.0]\nw = Variable(torch.Tensor([1.0]),  requires_grad=True)\n")])])]),t("p",[e._v("Calling the Backward function")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("l = loss(x_val, y_val)\nl.backward()\n")])])]),t("p",[t("strong",[e._v("Common Pitfalls")]),e._v("\nAs explained by this Blog Post by Radek, My friend and Mentor from the Fast AI community")]),e._v(" "),t("ul",[t("li",[e._v("Gradients accumulate everytime you call them, by default, be sure to call zero.gradient() to avoid that")]),e._v(" "),t("li",[e._v("Data Types, As mentioned in the Tensor Section, PyTorch supports various Tensor types. Be sure to check for the types to avoid Type compatibility errors.")])]),e._v(" "),t("p",[e._v("Feel free to ask any questions below.\nAlso drop us a comment on the tutorials that you’d love to read, I will try to have that up ASAP.")])])}),[],!1,null,null,null);a.default=r.exports}}]);