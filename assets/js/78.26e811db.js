(window.webpackJsonp=window.webpackJsonp||[]).push([[78],{468:function(e,t,s){"use strict";s.r(t);var a=s(9),n=Object(a.a)({},(function(){var e=this,t=e.$createElement,s=e._self._c||t;return s("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[s("p",[e._v("Newest PyTorch Lightning release includes the final API with better data decoupling, shorter logging syntax and tons of bug fixes")]),e._v(" "),s("p",[e._v("We’re happy to release PyTorch Lightning 0.9.0 today, which contains many great new features, more bug fixes than any release we ever had, but most importantly it introduced our mostly final API changes!")]),e._v(" "),s("p",[e._v("Lightning is being adopted by top researchers and AI labs around the world, and we are working hard to make sure we provide a smooth experience and support for all the latest best practices.\nIn this release, we are introducing two new major (and last) API changes:")]),e._v(" "),s("p",[s("strong",[e._v("LightningDataModules")])]),e._v(" "),s("p",[s("img",{attrs:{src:"https://miro.medium.com/max/700/1*vAMTBrmXPUtyvqR72i-wew.gif",alt:""}})]),e._v(" "),s("p",[e._v("Lightning is all about making your code more readable and structured.\nWe decouple the model architecture from engineering, and we continue to do the same with data. To make sharing and reusing data splits and transforms across projects, we created LightningDataModules.\nA LightningDataModule is a shareable, reusable class that encapsulates all the steps needed for training:")]),e._v(" "),s("ol",[s("li",[e._v("Download / tokenize / process.")]),e._v(" "),s("li",[e._v("Clean and save to disk for reuse.")]),e._v(" "),s("li",[e._v("Load inside Dataset in memory or just-in-time.")]),e._v(" "),s("li",[e._v("Apply transforms (rotate, tokenize, etc…).")]),e._v(" "),s("li",[e._v("Wrap inside a DataLoader.")])]),e._v(" "),s("p",[e._v("LightningDataModules can be shared and used anywhere:")]),e._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[e._v("import pytorch_lightning as pl\nfrom torch.utils.data import random_split, DataLoader\n\n# Note - you must have torchvision installed for this example\nfrom torchvision.datasets import MNIST\nfrom torchvision import transforms\n\n\nclass MNISTDataModule(pl.LightningDataModule):\n\n    def __init__(self, data_dir: str = './'):\n        super().__init__()\n        self.data_dir = data_dir\n        self.transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307,), (0.3081,))\n        ])\n\n        # self.dims is returned when you call dm.size()\n        # Setting default dims here because we know them.\n        # Could optionally be assigned dynamically in dm.setup()\n        self.dims = (1, 28, 28)\n\n    def prepare_data(self):\n        # download\n        MNIST(self.data_dir, train=True, download=True)\n        MNIST(self.data_dir, train=False, download=True)\n\n    def setup(self, stage=None):\n\n        # Assign train/val datasets for use in dataloaders\n        if stage == 'fit' or stage is None:\n            mnist_full = MNIST(self.data_dir, train=True, transform=self.transform)\n            self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n\n            # Optionally...\n            # self.dims = tuple(self.mnist_train[0][0].shape)\n\n        # Assign test dataset for use in dataloader(s)\n        if stage == 'test' or stage is None:\n            self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n\n            # Optionally...\n            # self.dims = tuple(self.mnist_test[0][0].shape)\n\n    def train_dataloader(self):\n        return DataLoader(self.mnist_train, batch_size=32)\n\n    def val_dataloader(self):\n        return DataLoader(self.mnist_val, batch_size=32)\n\n    def test_dataloader(self):\n        return DataLoader(self.mnist_test, batch_size=32)\n      \n\nmodel = LitClassifier()\ntrainer = Trainer()\nmnist = MNISTDataModule()\ntrainer.fit(model, mnist)\n\n")])])]),s("p",[e._v("In this video Nate Raw, DL research engineer at PyTorch Lightning, walks you step by step:")]),e._v(" "),s("p",[e._v("! "),s("a",{attrs:{href:"https://youtu.be/L---MBeSXFw",target:"_blank",rel:"noopener noreferrer"}},[s("OutboundLink")],1)]),e._v(" "),s("p",[e._v("Step results")]),e._v(" "),s("p",[e._v("We added to Lightning two new results objects: TrainResult and EvalResult. They are fancy dictionary objects to hold outputs from train/eval/test steps. They are meant to control where and when to log and how synchronization is done across accelerators:\nUse TrainResult to auto log from training_step:")]),e._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[e._v("# without val/test loop you can add model checkpoint or early stop\ndef training_step(self, batch, batch_idx):\n  x, y = batch\n  y_hat = self(x)\n  loss = F.cross_entropy(y_hat, y)\n  result = pl.TrainResult(loss, early_stop_on=loss, checkpoint_on=loss)\n  result.log('train_loss', loss)\n  return result\n\n")])])]),s("p",[e._v("The ‘train_loss’ we added to TrainResult will generate automatic tensorboard logs (you can also use any of the other loggers we support):")]),e._v(" "),s("p",[s("img",{attrs:{src:"https://miro.medium.com/max/700/1*PS99ZwxC0dfliCA6Tt9izA.png",alt:""}})]),e._v(" "),s("p",[e._v("TrainResult default is to log every step of training.")]),e._v(" "),s("p",[e._v("Use EvalResult to auto log from validation_step or test_step:")]),e._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[e._v("def validation_step(self, batch, batch_idx):\n  x, y = batch\n  y_hat = self(x)\n  loss = F.cross_entropy(y_hat, y)\n  result = EvalResult()\n  result.log('val_loss', loss)\n  return result\n\ndef test_step(self, batch, batch_idx):\n  x, y = batch\n  y_hat = self(x)\n  loss = F.cross_entropy(y_hat, y)\n  result = EvalResult()\n  result.log('test_loss', loss)\n  return result\n")])])]),s("p",[e._v("EvalResult default is to log every epoch end.")]),e._v(" "),s("p",[s("img",{attrs:{src:"https://miro.medium.com/max/700/1*K5zsUdjJdmC9iM_bY8RLzA.png",alt:""}})]),e._v(" "),s("p",[s("strong",[e._v("Sync across devices")])]),e._v(" "),s("p",[e._v("When training on multiple GPUs/CPUs/TPU cores, you can calculate the global mean of a logged metric as follows:")]),e._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[e._v("result.log('train_loss', loss, sync_dist=True)\n")])])]),s("p",[e._v("For more logging options, check out our docs.\nA few other highlights of 0.9 include:")]),e._v(" "),s("p",[e._v("*PyTorch 1.6 support\n*Added saving test predictions on multiple GPUs\n*Added support to export a model to ONNX format\n*More sklearn metrics, SSIM, BLEU\n*Added SyncBN for DDP\n*Support for remote directories via gfile")]),e._v(" "),s("p",[e._v("Read the full release notes here.")]),e._v(" "),s("p",[e._v("We also upgraded our docs with some videos that illustrate core Lightning features in seconds! Check them out, and let us know what you’d like to see next!")]),e._v(" "),s("p",[e._v("We want to thank all our devoted contributors for their hard work, and to the community for all your help. We definitely wouldn’t get here without you. Try it out, share your projects on our #slack, and stay tuned for our next release- 1.0!")])])}),[],!1,null,null,null);t.default=n.exports}}]);